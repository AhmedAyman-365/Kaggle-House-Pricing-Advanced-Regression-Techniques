# -*- coding: utf-8 -*-
"""House_Pricing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1m107dDYcnKfv-tAJ7EckDm8MWNxVSbCH

# **Importing the libraries**
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

"""# **Importing the dataset**"""

train = pd.read_csv('/content/train.csv')
test = pd.read_csv('/content/test.csv')

"""# **#(EDA) - Exploratory Data Analysis**"""

train.shape

test.shape

"""=> Saving the ID for the final submission then dropping it"""

train_ID = train['Id']
test_ID = test['Id']

train.drop('Id' , axis=1 , inplace=True)
test.drop('Id' , axis=1 , inplace=True)

"""Save the target variable "SalePrice" and dropping it from the training data"""

y_train = train['SalePrice']
X_train = train.drop('SalePrice' , axis=1)

"""=> Combining all the matrix of features into one matrix for easy cleaning"""

all_data = pd.concat([X_train , test]).reset_index(drop=True)
print(f"Combined data shape: {all_data.shape}")

"""#Handling missing values"""

# Set the size
plt.figure(figsize=(16, 6))

# Create the heatmap
sns.heatmap(all_data.isnull(), cbar=False, yticklabels=False , cmap='viridis')

# Add title
plt.title('Missing Values Heatmap')
plt.show()

"""Dropping the columns with more than 50% missing values"""

# Creating an empty list to put in it columns with more that 50% missing values
col_to_drop = []

# Creating a for loop to loop through all the columns and computing the missing percentage
for col in all_data.columns:
  missing_percentage = all_data[col].isnull().mean() * 100
  if missing_percentage > 50:
    col_to_drop.append(col)
    print(f"Dropping '{col}' -> missing percentage = {missing_percentage}% ")

# Dropping all the columns with missing values percentage greater than 50%
all_data.drop(columns=col_to_drop , inplace = True)

all_data.shape

"""Finding the column where missing actually means doesn't exist"""

# Columns where missing means "None" (the feature doesn't exist)
cols_where_missing_means_none = [
    'PoolQC', 'MiscFeature', 'Alley', 'Fence', 'FireplaceQu',
    'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond',
    'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'
]

for col in cols_where_missing_means_none:
    # Only fill if the column still exists in all_data
    if col in all_data.columns:
        all_data[col] = all_data[col].fillna("None")

"""Filling all the missing categorical data with mode and the missing numerical data with zero"""

for col in all_data.columns:
  if all_data[col].dtype == 'object':
    all_data[col] = all_data[col].fillna(all_data[col].mode()[0])
  else:
    all_data[col] = all_data[col].fillna(0)

all_data.isnull().sum().sum()

"""Lets fix the target variable (Label)"""

sns.distplot(y_train)

"""As we can see it is positive skewed we need to normalize it"""

# Use np.log1p which means "log(1 + x)" - handles zero values safely
y_train_logged = np.log1p(y_train)

# verify it looks better
sns.distplot(y_train_logged)

"""# **#Encoding categorical features**"""

all_data_encoded = pd.get_dummies(all_data)

all_data_encoded.shape

"""Making sure that X_train and y_train is compatable"""

# Get the number of rows in the original training data
n_train = len(train)

# Slice the first 'n_train' rows. This is your cleaned training data.
X_train_cleaned = all_data_encoded[:n_train]

# Slice the remaining rows starting from 'n_train'. This is your cleaned test data.
test_cleaned = all_data_encoded[n_train:]

# --- VERIFICATION (Important!) ---
print(f"Original y_train shape: {y_train_logged.shape}")
print(f"Cleaned X_train shape:  {X_train_cleaned.shape}")
print("-" * 30)
print(f"Original test shape:    {test.shape}")
print(f"Cleaned test shape:     {test_cleaned.shape}")

# The number of rows in y_train and X_train_cleaned MUST match.
assert y_train.shape[0] == X_train_cleaned.shape[0]
print("\nSuccess! Shapes match.")

# Create a copy of the cleaned features dataframes so we don't modify the original view
final_train_dataset = X_train_cleaned.copy()

# Add y_train into this dataframe as a new column named 'SalePrice'
final_train_dataset['SalePrice'] = y_train_logged

# --- VERIFICATION ---
print(f"Shape of final combined dataset: {final_train_dataset.shape}")

# Let's look at the last few columns to ensure 'SalePrice' is there at the end
print("\nLast 5 columns preview:")
print(final_train_dataset.iloc[:, -5:].head())

"""# **#Splitting the final_train_dataset data into X_train , y_train ,X_val , y_val**"""

from sklearn.model_selection import train_test_split
X_trn , X_val , y_trn , y_val = train_test_split(X_train_cleaned, y_train_logged, test_size=0.2, random_state=42)

"""# **#Scaling**"""

from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_trn = sc.fit_transform(X_trn)
X_val = sc.transform(X_val)

"""# **#Chossing the best regression model as the target variable is continous numbers**"""

from sklearn.linear_model import LinearRegression
regressor = LinearRegression()
regressor.fit(X_trn,y_trn)

"""# **#Prediction**"""

y_pred = regressor.predict(X_val)

"""# **#Evaluation**"""

from sklearn.metrics import r2_score

r2 = r2_score(y_val, y_pred)

print(f"R-squared (R2): {r2:.4f}")

"""# **#Applying K-fold cross-validation**"""

from sklearn.model_selection import cross_val_score
accuracies = cross_val_score(estimator = regressor, X = X_trn , y = y_trn , cv = 10)
print(accuracies.mean())
print(accuracies.std())

"""# **#Final submission**"""

# 1. CRITICAL STEP: Scale the test data using the SAME scaler you already fitted
# Do NOT call fit_transform again! Just call .transform()
test_scaled = sc.transform(test_cleaned)

# 2. Make predictions on the SCALED test data
# (These predictions will be in log-scale)
log_predictions = regressor.predict(test_scaled)

# 3. Reverse the log transform to get dollar prices
actual_predictions = np.expm1(log_predictions)

# 4. Create and save submission
submission_df = pd.DataFrame({
    'Id': test_ID,
    'SalePrice': actual_predictions
})

# I'll name it something different so you know it's the new one
submission_df.to_csv('submission_scaled_fixed.csv', index=False)

print("Fixed submission created! Upload 'submission_scaled_fixed.csv'.")